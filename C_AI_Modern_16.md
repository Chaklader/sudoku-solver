# C-16 | S-4: Making Complex Decisions

1. Sequential Decision Problems
    - Markov Decision Processes (MDPs)
    - Transition Models and Reward Functions
    - Utilities over Time and Discount Factors
    - Optimal Policies and State Utilities
    - Representing MDPs with Dynamic Decision Networks
2. Algorithms for MDPs
    - Value Iteration
    - Policy Iteration
    - Linear Programming Approaches
    - Online Algorithms for MDPs
    - Monte Carlo Tree Search for MDPs
3. Bandit Problems
    - The n-Armed Bandit Setting
    - The Bernoulli Bandit
    - Computing the Gittins Index
    - Exploration vs. Exploitation
    - Approximately Optimal Bandit Policies
4. Partially Observable MDPs
    - Definition of POMDPs
    - Belief States and Updates
    - Converting POMDPs to Belief-State MDPs
    - The Value of Information in POMDPs
    - Applications of POMDPs
5. Algorithms for Solving POMDPs
    - Value Iteration for POMDPs
    - Policy Loss and Convergence
    - Online Algorithms for POMDPs
    - Monte Carlo Planning for POMDPs
    - Partially Observable Monte Carlo Planning (POMCP)
