# C-25 | S-6: Deep Learning for Natural Language Processing

1. Word Embeddings
    - From One-Hot Vectors to Continuous Representations
    - Properties of Word Embedding Spaces
    - Applications of Word Embeddings
2. Recurrent Neural Networks for NLP
    - Language Models with RNNs
    - Classification with RNNs
    - LSTMs for NLP Tasks
    - Bidirectional RNNs
3. Sequence-to-Sequence Models
    - Basic Structure and Limitations
    - Attention Mechanisms
    - Decoding Methods (Greedy vs. Beam Search)
    - Applications in Machine Translation
4. The Transformer Architecture
    - Self-Attention
    - Positional Encoding
    - Transformer Encoder-Decoder Structure
5. Pretraining and Transfer Learning
    - Pretrained Word Embeddings
    - Pretrained Contextual Representations
    - Masked Language Models
    - Fine-Tuning for Specific Tasks
6. State of the Art
    - Language Models (GPT, T5)
    - Question Answering and Reading Comprehension
    - Limitations and Future Directions
